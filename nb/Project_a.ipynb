{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9586bcbe-87f8-4d51-acba-db676c09d021",
   "metadata": {},
   "source": [
    "# This file is to analyze the loan applications information in a HDFS (Hadoop Distributed File System) background. The HDFS now has two live datanodes. \n",
    "### Author: Xingjian (James) Tian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2b9f73-e162-4610-b088-8ae81f1d3a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyarrow as pa\n",
    "import pyarrow.fs\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00386158-6029-47d3-bf5f-e0e56c58519f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 51642105856 (48.10 GB)\n",
      "Present Capacity: 20826091520 (19.40 GB)\n",
      "DFS Remaining: 19731415040 (18.38 GB)\n",
      "DFS Used: 1094676480 (1.02 GB)\n",
      "DFS Used%: 5.26%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (2):\n",
      "\n",
      "Name: 172.19.0.2:9866 (project_hdfs-dn-2.project_hdfs_default)\n",
      "Hostname: fc4a5ec01e75\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "DFS Used: 547389440 (522.03 MB)\n",
      "Non DFS Used: 15391178752 (14.33 GB)\n",
      "DFS Remaining: 9865707520 (9.19 GB)\n",
      "DFS Used%: 2.12%\n",
      "DFS Remaining%: 38.21%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Tue Dec 24 19:16:08 GMT 2024\n",
      "Last Block Report: Tue Dec 24 16:35:55 GMT 2024\n",
      "Num of Blocks: 516\n",
      "\n",
      "\n",
      "Name: 172.19.0.5:9866 (project_hdfs-dn-1.project_hdfs_default)\n",
      "Hostname: 829a0ef3e8fa\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "DFS Used: 547287040 (521.93 MB)\n",
      "Non DFS Used: 15391281152 (14.33 GB)\n",
      "DFS Remaining: 9865707520 (9.19 GB)\n",
      "DFS Used%: 2.12%\n",
      "DFS Remaining%: 38.21%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Tue Dec 24 19:16:08 GMT 2024\n",
      "Last Block Report: Tue Dec 24 17:48:52 GMT 2024\n",
      "Num of Blocks: 516\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1: Check how many live DataNodes are in the cluster?\n",
    "!hdfs dfsadmin -fs hdfs://boss:9000 -report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652850ca-2391-484a-a465-553e00b255e0",
   "metadata": {},
   "source": [
    "##### For the output above, we could find that the output contains a line **\"Live datanodes (2):\"** which means that there are two live datanodes now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62dd1d-21d4-4e71-93d2-8b9b83801178",
   "metadata": {},
   "source": [
    "### Then, I just downloaded one file which is quite similar with the file we dealt with in CS544. The file can be accessed by https://ffiec.cfpb.gov/v2/data-browser-api/view/csv?states=WI&years=2024\n",
    "\n",
    "### The dataset include detailed information about mortgage loan applications, originations, denials, and more, making it useful for advanced analysis of mortgage lending trends and financial compliance. I have chosen to analyze loan applications from California for the year 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478e505b-7732-4f8d-920a-c49ee93a3889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘state_CA.csv’ already there; not retrieving.\n",
      "cp: `hdfs://boss:9000/single.csv': File exists\n",
      "cp: `hdfs://boss:9000/double.csv': File exists\n",
      "Found 2 items\n",
      "-rw-r--r--   2 root supergroup  360608549 2024-12-24 17:34 hdfs://boss:9000/double.csv\n",
      "-rw-r--r--   1 root supergroup  360608549 2024-12-24 17:34 hdfs://boss:9000/single.csv\n"
     ]
    }
   ],
   "source": [
    "#!hdfs dfs -rm -f hdfs://boss:9000/single.csv # Uncomment if needed\n",
    "#!hdfs dfs -rm -f hdfs://boss:9000/double.csv # Uncomment if needed\n",
    "\n",
    "# Downloads the dataset for California (state=CA) from the HMDA database for the year 2023.\n",
    "# The file is saved as \"state_CA.csv\" in the local directory.\n",
    "!wget -nc -O state_CA.csv \"https://ffiec.cfpb.gov/v2/data-browser-api/view/csv?states=CA&years=2023\"\n",
    "\n",
    "# Uploads the local file \"state_CA.csv\" to HDFS at \"hdfs://boss:9000/single.csv\".\n",
    "# `-D dfs.block.size=1048576`: Sets the block size to 1 MB for this upload.\n",
    "# `-D dfs.replication=1`: Sets the replication factor to 1 (no additional replicas).\n",
    "# The file is now stored in HDFS with one block replica.\n",
    "!hdfs dfs -D dfs.block.size=1048576 -D dfs.replication=1 -cp state_CA.csv hdfs://boss:9000/single.csv\n",
    "\n",
    "# Uploads the local file \"state_CA.csv\" to HDFS at \"hdfs://boss:9000/double.csv\".\n",
    "# `-D dfs.block.size=1048576`: Sets the block size to 1 MB for this upload.\n",
    "# `-D dfs.replication=2`: Sets the replication factor to 2 (two replicas for fault tolerance).\n",
    "# The file is now stored in HDFS with two block replicas.\n",
    "!hdfs dfs -D dfs.block.size=1048576 -D dfs.replication=2 -cp state_CA.csv hdfs://boss:9000/double.csv\n",
    "!hdfs dfs -ls hdfs://boss:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd98b18-e355-419c-87f0-dc2f4d148164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343.9 M  687.8 M  hdfs://boss:9000/double.csv\n",
      "343.9 M  343.9 M  hdfs://boss:9000/single.csv\n"
     ]
    }
   ],
   "source": [
    "# Q2: what are the logical and physical sizes of the CSV files?\n",
    "!hdfs dfs -du -h hdfs://boss:9000/double.csv\n",
    "!hdfs dfs -du -h hdfs://boss:9000/single.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015e94b-aa0c-4d74-8df0-36a936497178",
   "metadata": {},
   "source": [
    "##### Answer for **Q2**: The first columns show the logical and physical sizes. \n",
    "##### **single.csv**: Stored with a replication factor of 1, so physical size = logical size (343.9 M).\n",
    "##### **double.csv**: Stored with a replication factor of 2, so physical size = 2 × logical size (687.8 M)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ccef33-7ba2-4009-bf74-61067c257ac6",
   "metadata": {},
   "source": [
    "# WebHDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d41bc589-a6fe-41d8-9270-86e9fbb4ea3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FileStatus': {'accessTime': 1735066104880,\n",
       "  'blockSize': 1048576,\n",
       "  'childrenNum': 0,\n",
       "  'fileId': 16386,\n",
       "  'group': 'supergroup',\n",
       "  'length': 360608549,\n",
       "  'modificationTime': 1735061643582,\n",
       "  'owner': 'root',\n",
       "  'pathSuffix': '',\n",
       "  'permission': '644',\n",
       "  'replication': 1,\n",
       "  'storagePolicy': 0,\n",
       "  'type': 'FILE'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3: what is the file status for single.csv?\n",
    "\n",
    "# Bt defult, WebHDFS runs on port 9870.\n",
    "# The GETFILESTATUS operation retrieves metadata about a specific file in HDFS, such as its size, replication factor, block size, etc.\n",
    "r= requests.get(\"http://boss:9870/webhdfs/v1/single.csv?op=GETFILESTATUS\")\n",
    "r.raise_for_status()\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8345fb7-c15a-4d8d-929b-cbbd237d85db",
   "metadata": {},
   "source": [
    "##### Key fields explanation: \n",
    "##### blockSize: The block size of the file, which is **1 MB (1048576 bytes)**.\n",
    "##### length: The logical size of the file in bytes, which is about **343 MB**.\n",
    "##### replication: The replication factor of the file, which is 1 (no additional replicas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbca0115-374d-48b5-ae85-6de68c96a277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://fc4a5ec01e75:9864/webhdfs/v1/single.csv?op=OPEN&namenoderpcaddress=boss:9000&offset=0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4: what is the location for the first block of single.csv?\n",
    "\n",
    "# The OPEN operation retrieves a URL that points to the DataNode hosting the requested file or block.\n",
    "# Query Parameters: offset=0 specifies the offset (starting point) of the data to retrieve. \n",
    "# noredirect=true instructs the NameNode not to redirect to the actual DataNode URL directly, instead it provided the DataNode location as a response. \n",
    "r= requests.get(\"http://boss:9870/webhdfs/v1/single.csv?op=OPEN&offset=0&noredirect=true\")\n",
    "r.raise_for_status()\n",
    "r.json()['Location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386d5a38-ebc1-47d1-b3ac-9259f567002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc4a5ec01e75': 172, '829a0ef3e8fa': 172}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5: how are the blocks of single.csv distributed across the two DataNode containers?\n",
    "r= requests.get(\"http://boss:9870/webhdfs/v1/single.csv?op=GETFILEBLOCKLOCATIONS\")\n",
    "r.raise_for_status()\n",
    "info = r.json()[\"BlockLocations\"][\"BlockLocation\"]\n",
    "answer = {}\n",
    "# Iterate through each block location dictionary\n",
    "for dict in info:\n",
    "    # Extract the first host (DataNode) where the block is stored\n",
    "    host = dict[\"hosts\"][0]\n",
    "    if host in answer:\n",
    "        answer[host] +=1\n",
    "    else:\n",
    "        answer[host] = 1\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18516939-674a-46a6-8ce7-f33b0d3ff261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 19:16:28,217 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating a connection to HDFS using PyArrow, enabling me to perform many file operations in Python\n",
    "hdfs = pa.fs.HadoopFileSystem(\"boss\", 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2864d5c4-5dff-431a-ba98-76d62e6070a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'activity_y'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6: what are the first 10 bytes of single.csv?\n",
    "\n",
    "# Open the file \"single.csv\" stored in HDFS for reading.\n",
    "file = hdfs.open_input_file(\"hdfs://boss:9000/single.csv\")\n",
    "\n",
    "# Read the first 10 bytes of the file, starting at offset 0 (the beginning of the file).\n",
    "# Note: read_at(size, offset) reads 'size' bytes starting from the specified 'offset'.\n",
    "file.read_at(10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a2e08ba-6b09-4d49-b9c7-5c6a8f52a6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "940991"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7: how many lines of single.csv contain the string \"Single Family\"?\n",
    "\n",
    "\n",
    "with hdfs.open_input_file(\"hdfs://boss:9000/single.csv\") as f:\n",
    "    # Wrap the file object with a TextIOWrapper to read it as text.\n",
    "    # io.BufferedReader ensures efficient reading of the file in chunks.\n",
    "    reader = io.TextIOWrapper(io.BufferedReader(f))\n",
    "    pattern = r\"Single Family\"\n",
    "    count = 0\n",
    "    # Iterate through each line in the file, keeping track of line numbers using enumerate.\n",
    "    for i,line in enumerate(reader):\n",
    "        count += len(re.findall(pattern, line))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5329e-2ad4-4e8d-afcf-87a092d24867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
