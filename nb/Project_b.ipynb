{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8650f14e-751d-4897-8438-cb745f03f775",
   "metadata": {},
   "source": [
    "# Note: In this file, I want to simulate a situation where one DataNode is unavailable for unknown reasons.So, remember to manually kill one of the DataNode containers with a **docker kill** command!\n",
    "\n",
    "## Besides, if it still contains 2 live datanodes. Please wait a moment. It takes time to find one of the datanodes is off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ee0e82-671c-43ce-9ead-4832a4f6a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyarrow as pa\n",
    "import pyarrow.fs\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4edaf433-f0f4-4241-8d6a-e5ae1de21232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "Present Capacity: 10343915520 (9.63 GB)\n",
      "DFS Remaining: 9796628480 (9.12 GB)\n",
      "DFS Used: 547287040 (521.93 MB)\n",
      "DFS Used%: 5.29%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 344\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 172\n",
      "\tMissing blocks (with replication factor 1): 172\n",
      "\tLow redundancy blocks with highest priority to recover: 344\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 172.19.0.5:9866 (project_hdfs-dn-1.project_hdfs_default)\n",
      "Hostname: 829a0ef3e8fa\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "DFS Used: 547287040 (521.93 MB)\n",
      "Non DFS Used: 15460360192 (14.40 GB)\n",
      "DFS Remaining: 9796628480 (9.12 GB)\n",
      "DFS Used%: 2.12%\n",
      "DFS Remaining%: 37.94%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Tue Dec 24 19:34:23 GMT 2024\n",
      "Last Block Report: Tue Dec 24 17:48:52 GMT 2024\n",
      "Num of Blocks: 516\n",
      "\n",
      "\n",
      "Dead datanodes (1):\n",
      "\n",
      "Name: 172.19.0.2:9866 (172.19.0.2)\n",
      "Hostname: fc4a5ec01e75\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "DFS Used: 547389440 (522.03 MB)\n",
      "Non DFS Used: 15391195136 (14.33 GB)\n",
      "DFS Remaining: 9865691136 (9.19 GB)\n",
      "DFS Used%: 2.12%\n",
      "DFS Remaining%: 38.21%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Tue Dec 24 19:17:35 GMT 2024\n",
      "Last Block Report: Tue Dec 24 16:35:55 GMT 2024\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "!hdfs dfsadmin -fs hdfs://boss:9000 -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "441cf315-3892-43ec-912b-9861465865b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lost': 172, '829a0ef3e8fa': 172}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q9: how are the blocks of single.csv distributed across the DataNode containers?\n",
    "\n",
    "# The request fetches BlockLocations metadata from the HDFS Web API\n",
    "r= requests.get(\"http://boss:9870/webhdfs/v1/single.csv?op=GETFILEBLOCKLOCATIONS\")\n",
    "info = r.json()[\"BlockLocations\"][\"BlockLocation\"]\n",
    "answer = {}\n",
    "for dict in info:\n",
    "    if len(dict[\"hosts\"]) != 0:\n",
    "        host = dict[\"hosts\"][0]\n",
    "        if host in answer:\n",
    "            answer[host] +=1\n",
    "        else:\n",
    "            answer[host] = 1\n",
    "    else:\n",
    "        if 'lost' in answer:\n",
    "            answer['lost']+= 1\n",
    "        else:\n",
    "            answer['lost'] = 1\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4da0c4-ff2c-4023-af2e-5016118545f3",
   "metadata": {},
   "source": [
    "##### Lost Blocks: 172 blocks of single.csv are no longer accessible, as their DataNode is no longer available.\n",
    "##### Remaining Blocks: The surviving DataNode (829a0ef3e8fa) still holds 172 blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "828e8bf7-7fb1-4463-a280-abd3499e551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 19:34:26,702 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "hdfs = pa.fs.HadoopFileSystem(\"boss\", 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113aef23-64c3-4a52-8e07-6cd87e3ee453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2483"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q10: how many times does the text \"Single Family\" appear in the remaining blocks of single.csv?\n",
    "r= requests.get(\"http://boss:9870/webhdfs/v1/single.csv?op=GETFILEBLOCKLOCATIONS\")\n",
    "info = r.json()[\"BlockLocations\"][\"BlockLocation\"]\n",
    "for dict in info:\n",
    "    # Checks if the block is accessible by verifying whether the hosts list is non-empty.\n",
    "    # Blocks with an empty hosts list are considered lost or inaccessible.\n",
    "    if len(dict[\"hosts\"]) != 0:\n",
    "        offset = dict['offset']\n",
    "        count = 0\n",
    "        blksize = dict[\"length\"]\n",
    "        with hdfs.open_input_file(f\"hdfs://boss:9000/single.csv\") as f:\n",
    "            # Reads the content of the current block from the file based on its size (blksize) and starting point (offset).\n",
    "            blk=f.read_at(blksize, offset)\n",
    "            pattern = \"Single Family\"\n",
    "            count+=str(blk).count(pattern)\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d3bfd-7bfa-4a6d-873a-e0ea85e29bec",
   "metadata": {},
   "source": [
    "##### The result indicates that despite some blocks being inaccessible, the surviving blocks still contained significant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9633d-474f-4ee7-9399-535a98ec924c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
